{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob, urllib\n",
    "\n",
    "from scipy import integrate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import decimal as dc\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting constants and parameters\n",
    "\n",
    "For the presented simulations and most of the analyses, LAMMPS nano-units are used.\n",
    "see https://docs.lammps.org/units.html for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kB = 1.38064852e-2\n",
    "\n",
    "T = 310.0\n",
    "kT = kB * T\n",
    "\n",
    "# magnitude of external centripetal forces\n",
    "force = 0.4\n",
    "\n",
    "# number of bins along the reaction coordinate\n",
    "n_div = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change ```protein_id``` to access analyses corresponding to 0: MP$_1$ and 1: MP$_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_id = 1\n",
    "\n",
    "shiga_percent = 0.1\n",
    "\n",
    "protein_name = [\"MP_1\", \"MP_2\"][protein_id]\n",
    "protein_color = ['xkcd:slate blue', 'xkcd:pinkish'][protein_id]\n",
    "\n",
    "inverse_rc_label = r\"$1 / \\overline{r}_c$ [$\\mu$m$^{-1}$]\"\n",
    "\n",
    "print(f\"{protein_name} is chosen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"./data\"\n",
    "data_file_name =  f\"encapsulated_data_{protein_name}.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we check if the encapsulated data (which are accumulated from trajectories and simulation observables) are available for the chosen protein. If not, they are automatically downloaded from the ftp server into the ```./data``` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not glob.glob(f\"{data_folder}/{data_file_name}\"):\n",
    "    print(f\"downloading encapsulated data for {protein_name}...\")\n",
    "    urllib.request.urlretrieve(\n",
    "        f\"https://ftp.mi.fu-berlin.de/pub/cmb-data/steered_membrane_invagination_trajectories/{data_file_name}\",\n",
    "        f\"{data_folder}/{data_file_name}\")\n",
    "else:\n",
    "    print(f\"data found for {protein_name}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = np.load(f\"{data_folder}/{data_file_name}\", allow_pickle=True)\n",
    "\n",
    "sub_sample_step = 1\n",
    "burn_in = 1000\n",
    "\n",
    "_temp = {}\n",
    "\n",
    "for list_type in [\"shiga_pos\", \"q\", \"r_center\", \"conc\", \"time\", \"potential_energy\", \"u_red\", \"pressure\", \"box_dim\"]:\n",
    "    _temp[f'{list_type}_list'] = dat[f\"{list_type}_list\"]\n",
    "    \n",
    "\n",
    "for list_type in [\"shiga_pos\", \"q\", \"r_center\", \"time\", \"potential_energy\", \"u_red\", \"pressure\", \"box_dim\"]:\n",
    "    \n",
    "    globals()[f'{list_type}_list'] = []\n",
    "    \n",
    "    for _n in range(len(_temp[f'{list_type}_list'])):\n",
    "        \n",
    "        _x = _temp[f'{list_type}_list'][_n].copy()\n",
    "        \n",
    "        if len(_x) > burn_in * 1.5:\n",
    "            globals()[f'{list_type}_list'].append(_x[burn_in::sub_sample_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "total_time = 0.0\n",
    "\n",
    "for i, (_t, _rc) in enumerate(zip(time_list, r_center_list)):\n",
    "    \n",
    "    plt.plot(_t, 1.0e3 / _rc, label=f\"replica {i + 1}\")\n",
    "    \n",
    "    total_time += _t[-1]\n",
    "    \n",
    "plt.xlabel(\"time [ms]\")\n",
    "plt.ylabel(inverse_rc_label)\n",
    "plt.legend(ncols=3)\n",
    "\n",
    "print(f\"total simulated time = {total_time} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_min = 1.0e10\n",
    "rc_max = -1.0e10\n",
    "\n",
    "for _rc in r_center_list:\n",
    "    rc_min = min(rc_min, np.amin(_rc))\n",
    "    rc_max = max(rc_max, np.amax(_rc))\n",
    "\n",
    "print(f\"r_c range = {rc_min} -- {rc_max}\")\n",
    "global_rc_bins = np.linspace(rc_min * 1.03, rc_max * 0.97, n_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding trajectory weights\n",
    "\n",
    "To securely apply the Jarzynski equation, we need to make sure that trajectories start from microstates corresponding to the canonical distribution.\n",
    "\n",
    "The argument here is that if enough samples from the initial microstates of all trajectories are put together, they paint a better picture of the canonical ensemble. In the following cell, we use this information to fit an essentially Gaussian distribution to these samples and find how we should reweight each trajectory based on the ratio of individual samples to this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_energy_bins = 200\n",
    "\n",
    "initial_en_list = []\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "ensemble_en = []\n",
    "\n",
    "for _en in u_red_list:\n",
    "    for __en in _en[:1000]:\n",
    "        ensemble_en.append(__en)\n",
    "    \n",
    "ensemble_en = np.array(ensemble_en)\n",
    "en_ref = ensemble_en.mean()\n",
    "\n",
    "ensemble_en -= en_ref\n",
    "ens_std = ensemble_en.std()\n",
    "\n",
    "ens_hist, ens_bin_edges = np.histogram(ensemble_en, bins=n_energy_bins)\n",
    "ens_hist = ens_hist / np.trapz(ens_hist, ens_bin_edges[:-1])\n",
    "\n",
    "initital_en = []\n",
    "\n",
    "for _en in u_red_list:\n",
    "    initital_en.append(np.mean(_en[:10]) - en_ref)\n",
    "\n",
    "_prob = np.exp(-0.5 * (ens_bin_edges[1:] / ens_std) ** 2) / (np.sqrt(2.0 * np.pi) * ens_std)\n",
    "\n",
    "plt.plot(ens_bin_edges[1:], ens_hist, color=protein_color, label='dist. of accumulated samples')\n",
    "plt.plot(ens_bin_edges[1:], _prob, color='black', linestyle='--', label='fitted Gaussian distribution')\n",
    "\n",
    "init_en_hist, _ = np.histogram(initital_en, bins=ens_bin_edges)\n",
    "init_en_prob = init_en_hist / np.trapz(init_en_hist, ens_bin_edges[:-1])\n",
    "\n",
    "plt.plot(ens_bin_edges[1:], init_en_prob, color='xkcd:gray', label='current dist. of trajectories')\n",
    "\n",
    "plt.xlabel(\"energy [kT]\")\n",
    "plt.ylabel(\"probability density [1/kT]\")\n",
    "plt.legend()\n",
    "\n",
    "indices_of_hist = np.digitize(initital_en, ens_bin_edges[:-1], right=True) - 1\n",
    "\n",
    "traj_weights = init_en_prob[indices_of_hist] / _prob[indices_of_hist]\n",
    "traj_weights = traj_weights / np.sum(traj_weights)\n",
    "\n",
    "print(f\"trajectory weights = {traj_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding $\\frac{\\partial \\mathcal{H}_\\lambda}{\\partial \\lambda}$\n",
    "\n",
    "See the Methods section of the manuscript for full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_H_rc(shiga_pos, box_dim, force, r_center):\n",
    "    \n",
    "    n_steps, n_shiga = shiga_pos.shape[0:2]\n",
    "    \n",
    "    log_l = np.log(box_dim[:, 0])\n",
    "    \n",
    "    f = r_center * log_l\n",
    "\n",
    "    df_drc = np.zeros(n_steps)\n",
    "    df_drc[1:] = (f[1:] - f[:-1]) / (r_center[1:] - r_center[:-1])\n",
    "    \n",
    "    drc_l_over_drc = df_drc - log_l\n",
    "    \n",
    "    drc_rel = 1.0 - drc_l_over_drc\n",
    "    \n",
    "    return -force * float(n_shiga) * drc_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dH_drc_list = []\n",
    "\n",
    "for shiga_pos, box_dim, press, r_center, potential_energy in zip(shiga_pos_list,\n",
    "                                                                 box_dim_list,\n",
    "                                                                 pressure_list,\n",
    "                                                                 r_center_list,\n",
    "                                                                 potential_energy_list):\n",
    "    \n",
    "    n_steps, n_shiga = shiga_pos.shape[0:2]\n",
    "    \n",
    "    L_X, L_Y, L_Z = box_dim[:, 0], box_dim[:, 1], box_dim[:, 2]\n",
    "    A_XY = L_X * L_Y\n",
    "    \n",
    "    f1 = grad_H_rc(shiga_pos, box_dim, force, r_center)\n",
    "\n",
    "    # 0.1 factor to account for Bar to MPa change, which gives the force in pN units\n",
    "    dA_XY_drc = 2.0 * A_XY / r_center\n",
    "    p_XY = 0.1 * 0.5 * (press[:, 0] + press[:, 1])\n",
    "    \n",
    "    f2 = -p_XY * L_Z * dA_XY_drc\n",
    "\n",
    "    f = f1 + f2\n",
    "    \n",
    "    dH_drc_list.append(f.copy())\n",
    "\n",
    "    plt.plot(1.0e3 / r_center, f, '.', alpha=0.6, markeredgewidth=0)\n",
    "\n",
    "plt.xlabel(inverse_rc_label)\n",
    "plt.ylabel(r'${\\partial \\mathcal{H}_\\lambda}/{\\partial \\lambda} [pN]$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting trajectories\n",
    "\n",
    "Here, we use Monte Carlo sampling and bootstrap trajectories according to their weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstrap = 71000\n",
    "n_traj = len(r_center_list)\n",
    "\n",
    "current_state = np.random.randint(0, n_traj)\n",
    "\n",
    "state_list = []\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    \n",
    "    proposed_state = np.random.randint(0, n_traj)\n",
    "    \n",
    "    accept = False\n",
    "    \n",
    "    if traj_weights[proposed_state] > traj_weights[current_state]:\n",
    "        accept = True\n",
    "        \n",
    "    elif traj_weights[proposed_state] / traj_weights[current_state] > np.random.random_sample():\n",
    "        accept = True\n",
    "        \n",
    "    if accept:\n",
    "        current_state = proposed_state\n",
    "    \n",
    "    if i > 999:\n",
    "        if np.random.random_sample() > 0.95:\n",
    "            state_list.append(current_state)\n",
    "\n",
    "twbins = np.arange(n_traj + 1)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.hist(state_list, bins=twbins, alpha=0.3, density=True, width=0.4, label=\"Histogram of Monte Carlo samples\")\n",
    "plt.plot(twbins[:-1] + 0.2, traj_weights, '.r-', label=\"expected weights\")\n",
    "fig.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.xlabel('trajectory index')\n",
    "plt.ylabel('weight')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling external work along each trajectory\n",
    "\n",
    "Now we estimate the external weight along each bootstrapped trajectory sample, via integrating the corresponding $\\frac{\\partial \\mathcal{H}_\\lambda}{\\partial \\lambda}$ estimates along a global and constant range of reaction coordinate values. See the Methods section of the manuscript for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_W = []\n",
    "\n",
    "for i in range(n_div):\n",
    "    sampled_W.append([])\n",
    "\n",
    "bin_width = global_rc_bins[1] - global_rc_bins[0]\n",
    "\n",
    "def sample_in_rc_interval(r_center, rc_bin_right, rc_bin_left, dH_drc):\n",
    "\n",
    "    sel = (r_center < rc_bin_right) * (r_center >= rc_bin_left)\n",
    "\n",
    "    sampled_dH_drc = dH_drc[sel].copy()\n",
    "\n",
    "    if len(sampled_dH_drc) > 1:\n",
    "        _mu = np.mean(sampled_dH_drc)\n",
    "        _std = np.std(sampled_dH_drc)\n",
    "    else:\n",
    "        _mu = 0.0\n",
    "        _std = 1.0\n",
    "        \n",
    "    return sel, sampled_dH_drc, _mu, _std\n",
    "    \n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "for _state in tqdm(state_list):\n",
    "    \n",
    "    if np.random.random_sample() > 0.75:\n",
    "        dH_drc, r_center = dH_drc_list[_state], r_center_list[_state]\n",
    "\n",
    "        sampled_dH_drc_list = []\n",
    "        rc_W_bin_edges = []\n",
    "        \n",
    "        current_left = global_rc_bins[-2]\n",
    "        current_right = global_rc_bins[-1]\n",
    "        \n",
    "        while current_left > global_rc_bins[0]:\n",
    "\n",
    "            sel, sampled_dH_drc, _mu, _std = sample_in_rc_interval(r_center, current_right, current_left, dH_drc)\n",
    "            n_gathered_samples = np.sum(sel)\n",
    "            \n",
    "            valid = (np.abs(sampled_dH_drc - _mu) < 3.0 * _std)\n",
    "            n_valid = np.sum(valid)\n",
    "\n",
    "            if n_gathered_samples > 10 and n_valid / (n_gathered_samples + 1.0e-16) > 0.97:\n",
    "\n",
    "                    sampled_dH_drc_list.append(sampled_dH_drc[valid].copy())\n",
    "\n",
    "                    rc_W_bin_edges.append(current_left)\n",
    "\n",
    "                    current_right = current_left\n",
    "\n",
    "            current_left -= bin_width\n",
    "        \n",
    "        max_bin_index = len(sampled_dH_drc_list)\n",
    "\n",
    "        if max_bin_index > 0:\n",
    "\n",
    "            _y = np.zeros(max_bin_index)\n",
    "\n",
    "            for k in range(max_bin_index):\n",
    "\n",
    "                _y[k] = np.mean(np.random.choice(sampled_dH_drc_list[k], len(sampled_dH_drc_list[k]), replace=True))\n",
    "\n",
    "            W = integrate.cumulative_trapezoid(y=_y, x=rc_W_bin_edges, initial=0.0)\n",
    "\n",
    "            W_interp = np.interp(global_rc_bins, np.flip(rc_W_bin_edges), np.flip(W), left=1.0e19, right=1.0e19)\n",
    "\n",
    "            valid = W_interp < 1.0e19\n",
    "            plt.plot(1.0e3 / global_rc_bins[valid], W_interp[valid] / kT, color=protein_color, alpha=0.05, lw=0.5)\n",
    "\n",
    "            for j in range(n_div):\n",
    "                if valid[j]:\n",
    "                    sampled_W[j].append(W_interp[j])\n",
    "                    \n",
    "plt.xlabel(r\"$1 / \\overline{r}_c$ [$\\mu$m$^{-1}$]\")\n",
    "plt.ylabel(r\"$\\delta W / kT$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of the Jarzynski equation\n",
    "\n",
    "In the following, decimal arithmatic is used to handle extreme quantities arising from the exponentials in the Jarzynski equation. To regain some performance, we use a thread-pool for concurrent computation.\n",
    "\n",
    "Also, here, a second bootstrapping in invoked on samples of external work gathered for transition up to each reaction coordinate bin to estimate uncertainties in the estimated mean free energy differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_context = dc.Context(prec=200, rounding=dc.ROUND_HALF_EVEN)\n",
    "\n",
    "sampled_W = np.array(sampled_W, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstrap = 1000\n",
    "\n",
    "dG_Jar_sample = []\n",
    "\n",
    "def calc_dG_Jar (sampled_W):\n",
    "    \n",
    "    dG_Jar = np.zeros(n_div)\n",
    "    \n",
    "    for j in range(n_div):\n",
    "        \n",
    "        W_accum = sampled_W[j].copy()\n",
    "        \n",
    "        mean_exp_W_kT = dc.Decimal(0.0, context=dec_context)\n",
    "        n_samples_in_bin = len(W_accum)\n",
    "       \n",
    "        if n_samples_in_bin > 0:\n",
    "\n",
    "            for _W in np.random.choice(W_accum, n_samples_in_bin, replace=True):\n",
    "\n",
    "                mean_exp_W_kT += dc.Decimal(-_W / kT, context=dec_context).exp(context=dec_context)\n",
    "\n",
    "            mean_exp_W_kT = mean_exp_W_kT / dc.Decimal(float(n_samples_in_bin), context=dec_context)\n",
    "\n",
    "            dG_Jar[j] = -float(mean_exp_W_kT.ln(context=dec_context))\n",
    "            \n",
    "    return dG_Jar\n",
    "    \n",
    "pool = Pool()\n",
    "\n",
    "calc_thread = []\n",
    "\n",
    "for _n in range(n_bootstrap):\n",
    "    calc_thread.append(pool.apply_async(calc_dG_Jar, [sampled_W.copy()]))\n",
    "    \n",
    "for _c in tqdm(calc_thread):\n",
    "    dG_Jar_sample.append(_c.get())\n",
    "    \n",
    "pool.close ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dG_Jar = np.mean(dG_Jar_sample, axis=0)\n",
    "dG_Jar_err = np.std(dG_Jar_sample, axis=0)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(1.0e3 / global_rc_bins, dG_Jar, color=protein_color)\n",
    "plt.fill_between(1.0e3 / global_rc_bins, dG_Jar - 0.5 * dG_Jar_err, dG_Jar + 0.5 * dG_Jar_err, color=protein_color, alpha=0.1)\n",
    "plt.xlabel(inverse_rc_label)\n",
    "plt.ylabel(r\"$\\Delta G / kT$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving free energy data for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(f\"{data_folder}/Jarzynski_free_energy_{protein_name}\", rc_bins=global_rc_bins, dG=dG_Jar, dG_err=dG_Jar_err)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
